{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# BERT\n",
    "from transformers import BertTokenizer\n",
    "from transformers import TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_test = 20\n",
    "test_sentence = 'Test tokenization sentence. Followed by another sentence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_input = tokenizer.encode_plus(\n",
    "                        test_sentence,                      \n",
    "                        add_special_tokens = True, # add [CLS], [SEP]\n",
    "                        max_length = max_length_test, # max length of the text that can go to BERT\n",
    "                        pad_to_max_length = True, # add [PAD] tokens\n",
    "                        return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('encoded', bert_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Download IMDB reviews from ```tensorflow_datasets()```__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews', \n",
    "                                          split = (tfds.Split.TRAIN, tfds.Split.TEST),\n",
    "                                          as_supervised=True,\n",
    "                                          with_info=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Inspect info__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('info', ds_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Show examples__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review, label in tfds.as_numpy(ds_train.take(5)):\n",
    "    print('review', review.decode()[0:50], label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Helper functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_feature(review):\n",
    "    return tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=160, # truncates if len(s) > max_length\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True, # pads to the right by default\n",
    "        )\n",
    "\n",
    "# map to the expected input to TFBertForSequenceClassification, see here \n",
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "    return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,}, label\n",
    "\n",
    "def encode_examples(ds, limit=-1):\n",
    "    # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "    if (limit > 0):\n",
    "        ds = ds.take(limit)\n",
    "\n",
    "    for review, label in tfds.as_numpy(ds):\n",
    "        bert_input = convert_example_to_feature(review.decode())\n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        label_list.append([label])\n",
    "    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Preprocess dataset using helper functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "ds_train_encoded = encode_examples(ds_train).shuffle(10000).batch(32)\n",
    "# test dataset\n",
    "ds_test_encoded = encode_examples(ds_test).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define and compile model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
    "learning_rate = 2e-5\n",
    "# we will do just 1 epoch for illustration, though multiple epochs might be better as long as we will not overfit the model\n",
    "number_of_epochs = 1\n",
    "\n",
    "# model initialization\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# optimizer Adam recommended\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, \n",
    "                                     epsilon=1e-08)\n",
    "\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss=loss, \n",
    "              metrics=[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Train__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_history = model.fit(ds_train_encoded, \n",
    "                         epochs=number_of_epochs,\n",
    "                         batch_size=32,\n",
    "                         validation_data=ds_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Download pretrained model__\n",
    "\n",
    "Part of the TensorflowHub universe [here](https://tfhub.dev/see--/bert-uncased-tf2-qa/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('wget https://github.com/see--/natural-question-answering/releases/download/v0.0.1/tokenizer_tf2_qa.zip')\n",
    "os.system('unzip tokenizer_tf2_qa.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question answering function__\n",
    "\n",
    "Modified from an example found [here](https://tfhub.dev/see--/bert-uncased-tf2-qa/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('tokenizer_tf2_qa/vocab.txt')\n",
    "model = hub.load(\"https://tfhub.dev/see--/bert-uncased-tf2-qa/1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define paragraphs__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use markdown style paragraph tags to separate lines - ```<p>``` and ```</p>```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = '''<p>The computer is named Deep Thought.</p>.\n",
    "               <p>After 46 million years of training it found the answer.</p>\n",
    "               <p>The answer shocked everyone. It was 42!</p>'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define questions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = ['How long did it take to find the answer?',\n",
    "            'What was the answer to the great question?',\n",
    "            'What was the name of the computer?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer questions!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "        question_tokens = tokenizer.tokenize(question)\n",
    "        paragraph_tokens = tokenizer.tokenize(paragraph)\n",
    "        tokens = ['[CLS]'] + question_tokens + ['[SEP]'] + paragraph_tokens + ['[SEP]']\n",
    "        input_word_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(input_word_ids)\n",
    "        input_type_ids = [0] * (1 + len(question_tokens) + 1) + [1] * (len(paragraph_tokens) + 1)\n",
    "\n",
    "        input_word_ids, input_mask, input_type_ids = map(lambda t: tf.expand_dims(\n",
    "          tf.convert_to_tensor(t, dtype=tf.int32), 0), (input_word_ids, input_mask, input_type_ids))\n",
    "        outputs = model([input_word_ids, input_mask, input_type_ids])\n",
    "        # using `[1:]` will enforce an answer. `outputs[0][0][0]` is the ignored '[CLS]' token logit\n",
    "        short_start = tf.argmax(outputs[0][0][1:]) + 1\n",
    "        short_end = tf.argmax(outputs[1][0][1:]) + 1\n",
    "        answer_tokens = tokens[short_start: short_end + 1]\n",
    "        answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "        print(f'Question: {question}')\n",
    "        print(f'Answer: {answer}')\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Another example__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example from an article in [The Guardian](https://www.theguardian.com/environment/2021/apr/20/carbon-emissions-to-soar-in-2021-by-second-highest-rate-in-history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = '''<p>Carbon dioxide emissions are forecast to jump this year by the second biggest annual rise in history, as global economies pour stimulus cash into fossil fuels in the recovery from the Covid-19 recession.</p>\n",
    "                <p>The leap will be second only to the massive rebound 10 years ago after the financial crisis, and will put climate hopes out of reach unless governments act quickly, the International Energy Agency has warned.</p>\n",
    "                <p>Surging use of coal, the dirtiest fossil fuel, for electricity is largely driving the emissions rise, especially across Asia but also in the US. Coal’s rebound causes particular concern because it comes despite plunging prices for renewable energy, which is now cheaper than coal.</p>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"What is the problem?\",\n",
    "             \"Who has given this warning?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "        question_tokens = tokenizer.tokenize(question)\n",
    "        paragraph_tokens = tokenizer.tokenize(paragraph)\n",
    "        tokens = ['[CLS]'] + question_tokens + ['[SEP]'] + paragraph_tokens + ['[SEP]']\n",
    "        input_word_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(input_word_ids)\n",
    "        input_type_ids = [0] * (1 + len(question_tokens) + 1) + [1] * (len(paragraph_tokens) + 1)\n",
    "\n",
    "        input_word_ids, input_mask, input_type_ids = map(lambda t: tf.expand_dims(\n",
    "          tf.convert_to_tensor(t, dtype=tf.int32), 0), (input_word_ids, input_mask, input_type_ids))\n",
    "        outputs = model([input_word_ids, input_mask, input_type_ids])\n",
    "        # using `[1:]` will enforce an answer. `outputs[0][0][0]` is the ignored '[CLS]' token logit\n",
    "        short_start = tf.argmax(outputs[0][0][1:]) + 1\n",
    "        short_end = tf.argmax(outputs[1][0][1:]) + 1\n",
    "        answer_tokens = tokens[short_start: short_end + 1]\n",
    "        answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "        print(f'Question: {question}')\n",
    "        print(f'Answer: {answer}')\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Another example__\n",
    "\n",
    "This time taken from the introduction to the Wikipedia page for [Karl Marx](https://en.wikipedia.org/wiki/Karl_Marx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = '''Karl Heinrich Marx (German: [maʁks]; 5 May 1818 – 14 March 1883[13]) was a German philosopher, economist, historian, sociologist, political theorist, journalist and socialist revolutionary. Born in Trier, Germany, Marx studied law and philosophy at university. He married Jenny von Westphalen in 1843. Due to his political publications, Marx became stateless and lived in exile with his wife and children in London for decades, where he continued to develop his thought in collaboration with German thinker Friedrich Engels and publish his writings, researching in the reading room of the British Museum. His best-known titles are the 1848 pamphlet The Communist Manifesto and the three-volume Das Kapital (1867–1883). Marx's political and philosophical thought had enormous influence on subsequent intellectual, economic and political history. His name has been used as an adjective, a noun, and a school of social theory.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"Where was Marx born?\",\n",
    "             \"Who was his main collaborator?\",\n",
    "             \"How do you pronounce Marx in German?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "        question_tokens = tokenizer.tokenize(question)\n",
    "        paragraph_tokens = tokenizer.tokenize(paragraph)\n",
    "        tokens = ['[CLS]'] + question_tokens + ['[SEP]'] + paragraph_tokens + ['[SEP]']\n",
    "        input_word_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(input_word_ids)\n",
    "        input_type_ids = [0] * (1 + len(question_tokens) + 1) + [1] * (len(paragraph_tokens) + 1)\n",
    "\n",
    "        input_word_ids, input_mask, input_type_ids = map(lambda t: tf.expand_dims(\n",
    "          tf.convert_to_tensor(t, dtype=tf.int32), 0), (input_word_ids, input_mask, input_type_ids))\n",
    "        outputs = model([input_word_ids, input_mask, input_type_ids])\n",
    "        # using `[1:]` will enforce an answer. `outputs[0][0][0]` is the ignored '[CLS]' token logit\n",
    "        short_start = tf.argmax(outputs[0][0][1:]) + 1\n",
    "        short_end = tf.argmax(outputs[1][0][1:]) + 1\n",
    "        answer_tokens = tokens[short_start: short_end + 1]\n",
    "        answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "        print(f'Question: {question}')\n",
    "        print(f'Answer: {answer}')\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
